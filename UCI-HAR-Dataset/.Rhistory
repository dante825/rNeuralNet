# Cleaning the levels of the factor of Item_Fat_Content
sum(martData$Item_Fat_Content=='LF')
sum(martData$Item_Fat_Content=='low fat')
martData$Item_Fat_Content[martData$Item_Fat_Content=='low fat'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='reg')
martData$Item_Fat_Content[martData$Item_Fat_Content=='reg'] <- 'Regular'
martData$Item_Fat_Content <- factor(martData$Item_Fat_Content)
levels(martData$Item_Fat_Content)
# Cleaning the levels of the factor Outlet_Size
levels(martData$Outlet_Size) <- c('Unknown', 'High', 'Medium', 'Small')
# Clustering is unsupervised so remove the response variable and the identifiers (which are not variables)
clusData <- martData %>% select(Item_Weight, Item_Fat_Content, Item_Type, Item_Visibility, Item_MRP,
Outlet_Size, Outlet_Type, Outlet_Location_Type)
############### Data Preprocessing ###################s
library(dplyr)
# Clustering is unsupervised so remove the response variable and the identifiers (which are not variables)
clusData <- martData %>% select(Item_Weight, Item_Fat_Content, Item_Type, Item_Visibility, Item_MRP,
Outlet_Size, Outlet_Type, Outlet_Location_Type)
colnames(clusData)
str(clusData)
############ Principal Component Analysis (PCA) #############
## Convert the categorical variables into continuous variables
library(dummies)
dummyDf <- dummy.data.frame(clusData, names = c('Item_Fat_Content', 'Item_Type','Outlet_Size',
'Outlet_Type', 'Outlet_Location_Type'))
# All the data is in numeric form
str(dummyDf)
impFeatures <- prcomp(dummyDf, scale. = T)
names(impFeatures)
# Center and Scale = means and std of the variables
impFeatures$center
impFeatures$scale
# Rotation = principal component loading, most important features
impFeatures$rotation
head(impFeatures$rotation)
dim(impFeatures$rotation)
dim(impFeatures$x)
biplot(impFeatures, scale=0)
# Compute the standard deviation for each of the component
std_dev <- impFeatures$sdev
variance <- std_dev^2
# Check the variance for the first 10 components
head(variance, 10)
# Proportion of variance explained
# The higher the percentage the more important the feature
propVariance <- variance/sum(variance)
propVariance[1:20]
# How many of these feature to select?
# Scree plot
plot(propVariance, xlab = 'Principal Component', ylab = 'Proportion of Variance Explained', type = 'b',
main = 'Scree Plot of proportion of variance')
# Clustering Algorithm Comparison
# Loading the data
trainingPath <- file.path('./bigMartTrain.csv')
martData <- read.csv(trainingPath)
dim(martData)
head(martData)
str(martData)
############### Data Preprocessing ###################s
library(dplyr)
# Checking the NA values
sum(is.na(martData))
sum(is.na(martData$Item_Weight))
# Impute the NAs with the median
martData$Item_Weight[is.na(martData$Item_Weight)] <- median(martData$Item_Weight, na.rm=T)
# No more NA after inferring the mean to the NAs
sum(is.na(martData))
# Some of the item visibility has the value of 0 which indicate some missing data
sum(martData$Item_Visibility == 0)
# Impute them with median
martData$Item_Visibility <- ifelse(martData$Item_Visibility == 0, median(martData$Item_Visibility),
martData$Item_Visibility)
# Cleaning the levels of the factor
sum(martData$Item_Fat_Content=='LF')
martData$Item_Fat_Content[martData$Item_Fat_Content=='LF'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='low fat')
martData$Item_Fat_Content[martData$Item_Fat_Content=='low fat'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='reg')
martData$Item_Fat_Content[martData$Item_Fat_Content=='reg'] <- 'Regular'
martData$Item_Fat_Content <- factor(martData$Item_Fat_Content)
levels(martData$Item_Fat_Content)
# Cleaning the levels of the factor Outlet_Size
levels(martData$Outlet_Size) <- c('Unknown', 'High', 'Medium', 'Small')
# Clustering is unsupervised so remove the response variable and the identifiers (which are not variables)
# Just keep the variables in numeric form
clusData <- martData %>% select(Item_Weight, Item_Visibility, Item_MRP)
colnames(clusData)
str(clusData)
dim(clusData)
################## Hierarchical clustering ##################
library(cluster)
# Using Euclidean distance on the data with numerical values
hc <- hclust(d = dist(clusData, method = 'euclidean'), method = 'ward.D')
plot(hc, main = 'Dendrogram with Euclidean distance', xlab = 'Items', ylab = 'Euclidean distances')
# Get the number of cluster based on the dendogram
rect.hclust(hc, k = 4, border = "red")
y_hc = cutree(hc, 4)
# Cluster membership
table(y_hc)
clusplot(clusData, y_hc, lines=0, shade=TRUE, color=TRUE, plotchar=FALSE, span=TRUE,
main='Hierarchical clustering of Big Mart Sales data', xlab='X', ylab='Y')
########### Hierarchical Clustering with Categorical Values ###########
# Includes some categorical values in the data
hierData <- martData %>% select(Item_MRP, Item_Weight, Item_Visibility, Item_Fat_Content, Item_Type, Outlet_Type,
Outlet_Size)
# Use gower distance instead of euclidean to calculate the similarity
gowerDist <- daisy(hierData, metric = 'gower')
# Aggloromerative clustering Dendogram
hc <- hclust(gowerDist, method = 'complete')
plot(hc, main = 'Agglomerative, complete linkage, Gower distance', xlab = 'Items', ylab = 'Gower distance')
# Get the number of clusters based on the dendogram
rect.hclust(hc, k = 4, border = "red")
y_hc = cutree(hc, 4)
# Cluster membership
table(y_hc)
# Visualising the clusters
clusplot(hierData, y_hc, lines = 0, shade = TRUE, color = TRUE, plotchar = FALSE, span = TRUE,
main = "Hierarchical clustering with Gower's distance metric", xlab = 'X', ylab = 'Y')
# Clustering Algorithm Comparison
# Loading the data
trainingPath <- file.path('./bigMartTrain.csv')
martData <- read.csv(trainingPath)
dim(martData)
head(martData)
############### Data Preprocessing ###################s
library(dplyr)
# Checking the NA values
sum(is.na(martData))
sum(is.na(martData$Item_Weight))
# Impute the NAs with the median
martData$Item_Weight[is.na(martData$Item_Weight)] <- median(martData$Item_Weight, na.rm=T)
# No more NA after inferring the mean to the NAs
sum(is.na(martData))
# Some of the item visibility has the value of 0 which indicate some missing data
sum(martData$Item_Visibility == 0)
# Impute them with median
martData$Item_Visibility <- ifelse(martData$Item_Visibility == 0, median(martData$Item_Visibility),
martData$Item_Visibility)
# Cleaning the levels of the factor of Item_Fat_Content
sum(martData$Item_Fat_Content=='LF')
martData$Item_Fat_Content[martData$Item_Fat_Content=='LF'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='low fat')
martData$Item_Fat_Content[martData$Item_Fat_Content=='low fat'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='reg')
martData$Item_Fat_Content[martData$Item_Fat_Content=='reg'] <- 'Regular'
martData$Item_Fat_Content <- factor(martData$Item_Fat_Content)
levels(martData$Item_Fat_Content)
# Cleaning the levels of the factor Outlet_Size
levels(martData$Outlet_Size) <- c('Unknown', 'High', 'Medium', 'Small')
# Clustering is unsupervised so remove the response variable and the identifiers (which are not variables)
clusData <- martData %>% select(Item_Weight, Item_Fat_Content, Item_Type, Item_Visibility, Item_MRP,
Outlet_Size, Outlet_Type, Outlet_Location_Type)
colnames(clusData)
str(clusData)
############ Principal Component Analysis (PCA) #############
## Convert the categorical variables into continuous variables
library(dummies)
dummyDf <- dummy.data.frame(clusData, names = c('Item_Fat_Content', 'Item_Type','Outlet_Size',
'Outlet_Type', 'Outlet_Location_Type'))
# All the data is in numeric form
str(dummyDf)
impFeatures <- prcomp(dummyDf, scale. = T)
names(impFeatures)
# Center and Scale = means and std of the variables
impFeatures$center
impFeatures$scale
# Rotation = principal component loading, most important features
impFeatures$rotation
head(impFeatures$rotation)
dim(impFeatures$rotation)
dim(impFeatures$x)
# Compute the standard deviation for each of the component
std_dev <- impFeatures$sdev
variance <- std_dev^2
# Check the variance for the first 10 components
head(variance, 10)
# Proportion of variance explained
# The higher the percentage the more important the feature
propVariance <- variance/sum(variance)
propVariance[1:20]
# How many of these feature to select?
# Scree plot
plot(propVariance, xlab = 'Principal Component', ylab = 'Proportion of Variance Explained', type = 'b',
main = 'Scree Plot of proportion of variance')
# Confirmation check with a cumulative variance plot
plot(cumsum(propVariance), xlab = 'Principal Component', ylab = 'Cumulative Proportion of Variance Explained',
type = 'b', main = 'Cumulative Proportion of Variance')
train2 <- data.frame(impFeatures$x)
train2 <- train2[,1:25]
###################### K-means ####################
# Elbow method to detect the best number of clusters for K-means
set.seed(123)
vec <- vector()
for (i in 1:10) {
vec[i] = sum(kmeans(train2, i)$withinss)
}
plot(x = 1:10, y = vec, type = 'b', main = 'The Elbow Method', xlab = 'Number of Clusters', ylab = 'WCSS')
# Fitting kmeans to the dataset
library(cluster)
set.seed(123)
kmeans <- kmeans(x = train2, centers = 8)
ykmeans <- kmeans$cluster
# Cluster membership
table(ykmeans)
train2
# Visualizing the clusters
clusplot(train2, ykmeans, lines = 0, shade = T, color = T, plotchar = F, span = T,
main = 'K-means clustering with Big Mart Sales data', xlab = 'X', ylab = 'Y')
################## Hierarchical clustering ##################
# Using the dendrogram to find the optimal number of clusters
hc = hclust(d = dist(train2, method = 'euclidean'), method = 'ward.D')
plot(hc, main = 'Dendrogram', xlab = 'Items', ylab = 'Euclidean distances')
# Fitting Hierarchical Clustering to the dataset
rect.hclust(hc, k = 3, border = "red")
y_hc = cutree(hc, 3)
table(y_hc)
# Visualising the clusters
library(cluster)
clusplot(train2, y_hc, lines = 0, shade = TRUE, color = TRUE, plotchar = FALSE, span = TRUE,
main = 'Hierarchical clustering of the Big Mart Sales data', xlab = 'X', ylab = 'Y')
# Clustering Algorithm Comparison
# Loading the data
trainingPath <- file.path('./bigMartTrain.csv')
martData <- read.csv(trainingPath)
dim(martData)
############### Data Preprocessing ###################s
library(dplyr)
# Checking the NA values
sum(is.na(martData))
sum(is.na(martData$Item_Weight))
########### Hierarchical Clustering with Categorical Values ###########
# Includes some categorical values in the data
hierData <- martData %>% select(Item_Fat_Content, Item_Type, Outlet_Type,
Outlet_Size)
# Use gower distance instead of euclidean to calculate the similarity
gowerDist <- daisy(hierData, metric = 'gower')
library(cluster)
# Use gower distance instead of euclidean to calculate the similarity
gowerDist <- daisy(hierData, metric = 'gower')
# Aggloromerative clustering Dendogram
hc <- hclust(gowerDist, method = 'complete')
plot(hc, main = 'Agglomerative, complete linkage, Gower distance', xlab = 'Items', ylab = 'Gower distance')
# Get the number of clusters based on the dendogram
rect.hclust(hc, k = 4, border = "red")
plot(hc, main = 'Agglomerative, complete linkage, Gower distance', xlab = 'Items', ylab = 'Gower distance')
# Get the number of clusters based on the dendogram
rect.hclust(hc, k = 6, border = "red")
y_hc = cutree(hc, 6)
# Cluster membership
table(y_hc)
# Visualising the clusters
clusplot(hierData, y_hc, lines = 0, shade = TRUE, color = TRUE, plotchar = FALSE, span = TRUE,
main = "Hierarchical clustering with Gower's distance metric", xlab = 'X', ylab = 'Y')
# Clustering Algorithm Comparison
# Loading the data
trainingPath <- file.path('./bigMartTrain.csv')
martData <- read.csv(trainingPath)
dim(martData)
head(martData)
str(martData)
############### Data Preprocessing ###################s
library(dplyr)
# Checking the NA values
sum(is.na(martData))
sum(is.na(martData$Item_Weight))
# Impute the NAs with the median
martData$Item_Weight[is.na(martData$Item_Weight)] <- median(martData$Item_Weight, na.rm=T)
# No more NA after inferring the mean to the NAs
sum(is.na(martData))
# Some of the item visibility has the value of 0 which indicate some missing data
sum(martData$Item_Visibility == 0)
# Impute them with median
martData$Item_Visibility <- ifelse(martData$Item_Visibility == 0, median(martData$Item_Visibility),
martData$Item_Visibility)
# Cleaning the levels of the factor of Item_Fat_Content
sum(martData$Item_Fat_Content=='LF')
martData$Item_Fat_Content[martData$Item_Fat_Content=='LF'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='low fat')
martData$Item_Fat_Content[martData$Item_Fat_Content=='low fat'] <- 'Low Fat'
sum(martData$Item_Fat_Content=='reg')
martData$Item_Fat_Content[martData$Item_Fat_Content=='reg'] <- 'Regular'
martData$Item_Fat_Content <- factor(martData$Item_Fat_Content)
levels(martData$Item_Fat_Content)
# Cleaning the levels of the factor Outlet_Size
levels(martData$Outlet_Size) <- c('Unknown', 'High', 'Medium', 'Small')
#################### Data Selection ############################
# Clustering is unsupervised so remove the response variable and the identifiers (which are not variables)
clusData <- martData %>% select(Item_Weight, Item_Fat_Content, Item_Type, Item_Visibility, Item_MRP,
Outlet_Size, Outlet_Type, Outlet_Location_Type)
colnames(clusData)
str(clusData)
############ Principal Component Analysis (PCA) #############
## Convert the categorical variables into continuous variables
library(dummies)
dummyDf <- dummy.data.frame(clusData, names = c('Item_Fat_Content', 'Item_Type','Outlet_Size',
'Outlet_Type', 'Outlet_Location_Type'))
# All the data is in numeric form
str(dummyDf)
impFeatures <- prcomp(dummyDf, scale. = T)
names(impFeatures)
# Center and Scale = means and std of the variables
impFeatures$center
impFeatures$scale
# Rotation = principal component loading, most important features
impFeatures$rotation
head(impFeatures$rotation)
dim(impFeatures$rotation)
dim(impFeatures$x)
biplot(impFeatures, scale=0)
# Compute the standard deviation for each of the component
std_dev <- impFeatures$sdev
variance <- std_dev^2
# Check the variance for the first 10 components
head(variance, 10)
# Proportion of variance explained
# The higher the percentage the more important the feature
propVariance <- variance/sum(variance)
propVariance[1:20]
# How many of these feature to select?
# Scree plot
plot(propVariance, xlab = 'Principal Component', ylab = 'Proportion of Variance Explained', type = 'b',
main = 'Scree Plot of proportion of variance')
# Confirmation check with a cumulative variance plot
plot(cumsum(propVariance), xlab = 'Principal Component', ylab = 'Cumulative Proportion of Variance Explained',
type = 'b', main = 'Cumulative Proportion of Variance')
train2 <- data.frame(impFeatures$x)
train2 <- train2[,1:25]
###################### K-means ####################
# Elbow method to detect the best number of clusters for K-means
set.seed(123)
# Clustering Algorithm Comparison
# Loading the data
trainingPath <- file.path('./bigMartTrain.csv')
martData <- read.csv(trainingPath)
dim(martData)
head(martData)
library(dplyr)
library(dplyr)
setwd('./UCI-HAR-Dataset/')
################ Loading the datasets ###############
# Load the features names
features <- read.delim('./features.txt', header = F, sep = '')
dim(features)
head(features)
# Select the features names from the loaded df
features <- features %>% select(V2)
# Load the training dataset
trainData <- read.delim('./train/X_train.txt', header = F, sep = '', dec = '.', col.names = features[,1])
dim(trainData)
head(trainData)
# Load the label for the training data
trainLabel <- read.delim('./train/y_train.txt', header = F, sep = '', col.names = 'activity')
dim(trainLabel)
# Add the label data into a column of the training data
trainData$activity <- trainLabel[,1]
# Load the test data
testData <- read.delim('./test/X_test.txt', header = F, sep = '', dec = '.', col.names = features[,1])
dim(testData)
# Load the labels for test data
testLabel <- read.delim('./test/y_test.txt', header = F, sep = '', col.names = 'activity')
dim(testLabel)
# Add the test labels to the test data
testData$activity <- testLabel[,1]
########### Classification #############
library(neuralnet)
library(caret)
n <- names(trainData)
f <- as.formula(paste('activity ~', paste( n[!n %in% 'activity'], collapse = '+')))
nn <- neuralnet(f, trainData, hidden = 4, linear.output = FALSE, threshold = 0.01)
plot(nn, rep = 'best')
# Remove the labels from the test data
testNolabel <- testData %>% select(-activity)
# Test the neural network model on the test data
nn.results <- compute(nn, testNolabel)
# Confusion Matrix
# result <- data.frame(actual = testData$activity, prediction = round(nn.results$net.result))
# t <- table(result)
# confusionMatrix(t)
prediction <- round(nn.results$net.result)
table(prediction)
actual <- testData$activity
table(actual)
u <- union(prediction, actual)
t <- table(factor(prediction, u), factor(actual, u))
confusionMatrix(t)
?prcomp
##### PCA #########
pcaTrain <- trainData %>% select(-activity)
##### PCA #########
pcaTrain <- trainData %>% select(-activity)
pcaTest <- testData %>% select(-activity)
pca <- prcomp(pcaTrain, scale = F)
# Variance
pcaVar <- (pca$sdev) ^2
# Proportion of variance
propVar <- pcaVar / sum(pcaVar)
# Plot the scree plot for proportion of variance
plot(propVar, xlab = 'Principal Component', ylab = 'Proportion of variance explained', type = 'b',
main = 'Scree Plot for Principal Components')
# Sanity check with cumulative scree plot
plot(cumsum(propVar), xlab = 'Principal component', ylab = 'Cumulative proportion of variance explained',
type = 'b', main = 'Cumulative Proportion of Variance explained by Principal Component')
pca$x
# Create a new dataset from PCA result
train2 <- data.frame(class = trainData$activity, pca$x)
t <- as.data.frame(predict(pca, newdata = pcaTest))
# Create a new dataset from PCA result
tmpTrain <- data.frame(class = trainData$activity, pca$x)
train2 <- train[,1:100]
train2 <- tmpTrain[,1:100]
ncol(tmpTrain)
ncol(train2 )
?predict
head(train2)
train2 <- tmpTrain[,1:101]
t
test2 <- t[, 1:100]
n <- names(train2)
f <- as.formula(paste('activity ~', paste(n[!n %in% 'activity'], collapse = '+')))
nn <- neuralnet(f, train2, hidden = 4, linear.output = FALSE, threshold = 0.01)
f <- as.formula(paste('class ~', paste(n[!n %in% 'activity'], collapse = '+')))
nn <- neuralnet(f, train2, hidden = 4, linear.output = FALSE, threshold = 0.01)
f <- as.formula(paste('activity ~', paste(n[!n %in% 'activity'], collapse = '+')))
# Create a new dataset from PCA result
tmpTrain <- data.frame(activity = trainData$activity, pca$x)
t <- as.data.frame(predict(pca, newdata = pcaTest))
train2 <- tmpTrain[,1:101]
test2 <- t[, 1:100]
# Build the neural network
library(neuralnet)
n <- names(train2)
f <- as.formula(paste('activity ~', paste(n[!n %in% 'activity'], collapse = '+')))
nn <- neuralnet(f, train2, hidden = 4, linear.output = FALSE, threshold = 0.01)
plot(nn, rep = 'best')
# Test the model
nn.results <- compute(nn, test2)
results <- data.frame(actual=testData$activity, prediction = round(nn.results$net.result))
t <- table(results)
t
confusionMatrix(t)
confusionMatrix(t)
prediction <- round(nn.results$net.result)
table(prediction)
actual <- testData$activity
table(actual)
results
t <- table(results)
t
table(actual)
prediction <- round(nn.results$net.result)
t <- table(results)
results
# Test the model
nn.results2 <- compute(nn, test2)
results <- data.frame(actual=testData$activity, prediction = round(nn.results2$net.result))
t <- table(results)
confusionMatrix(t)
prediction <- round(nn.results2$net.result)
table(prediction)
actual <- testData$activity
table(actual)
u <- union(prediction, actual)
t <- table(factor(prediction, u), factor(actual, u))
confusionMatrix(t)
propVar[1:10]
propVar[1:100]
# Plot the scree plot for proportion of variance
plot(propVar, xlab = 'Principal Component', ylab = 'Proportion of variance explained', type = 'b',
main = 'Proportion of Variance explained by Principal Components')
# Sanity check with cumulative scree plot
plot(cumsum(propVar), xlab = 'Principal component', ylab = 'Cumulative proportion of variance explained',
type = 'b', main = 'Cumulative Proportion of Variance explained by Principal Component')
# Create a new dataset from PCA result
tmpTrain <- data.frame(activity = trainData$activity, pca$x)
train2 <- tmpTrain[,1:51]
predict(pca, newdata = pcaTest)
# Create a new dataset from PCA result
tmpTrain <- data.frame(activity = trainData$activity, pca$x)
t <- as.data.frame(predict(pca, newdata = pcaTest))
train2 <- tmpTrain[,1:51]
test2 <- t[, 1:50]
# Build the neural network
library(neuralnet)
library(caret)
n <- names(train2)
n
f <- as.formula(paste('activity ~', paste(n[!n %in% 'activity'], collapse = '+')))
f
nn <- neuralnet(f, train2, hidden = 4, linear.output = FALSE, threshold = 0.01)
plot(nn, rep = 'best')
# Test the model
nn.results2 <- compute(nn, test2)
nn.results2
prediction <- round(nn.results2$net.result)
table(prediction)
actual <- testData$activity
table(actual)
u <- union(prediction, actual)
t <- table(factor(prediction, u), factor(actual, u))
confusionMatrix(t)
nn <- neuralnet(f, train2, hidden = 5, linear.output = FALSE, threshold = 0.01)
plot(nn, rep = 'best')
# Test the model
nn.results2 <- compute(nn, test2)
?compute
prediction <- round(nn.results2$net.result)
table(prediction)
actual <- testData$activity
table(actual)
u <- union(prediction, actual)
t <- table(factor(prediction, u), factor(actual, u))
confusionMatrix(t)
